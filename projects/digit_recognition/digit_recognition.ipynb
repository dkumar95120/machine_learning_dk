{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Engineer Nanodegree\n",
    "## Deep Learning\n",
    "## Project: Build a Digit Recognition Program\n",
    "\n",
    "In this notebook, a template is provided for you to implement your functionality in stages which is required to successfully complete this project. If additional code is required that cannot be included in the notebook, be sure that the Python code is successfully imported and included in your submission, if necessary. Sections that begin with **'Implementation'** in the header indicate where you should begin your implementation for your project. Note that some sections of implementation are optional, and will be marked with **'Optional'** in the header.\n",
    "\n",
    "In addition to implementing code, there will be questions that you must answer which relate to the project and your implementation. Each section where you will answer a question is preceded by a **'Question'** header. Carefully read each question and provide thorough answers in the following text boxes that begin with **'Answer:'**. Your project submission will be evaluated based on your answers to each of the questions and the implementation you provide.\n",
    "\n",
    ">**Note:** Code and Markdown cells can be executed using the **Shift + Enter** keyboard shortcut. In addition, Markdown cells can be edited by typically double-clicking the cell to enter edit mode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Step 1: Design and Test a Model Architecture\n",
    "Design and implement a deep learning model that learns to recognize sequences of digits. Train the model using synthetic data generated by concatenating character images from [notMNIST](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html) or [MNIST](http://yann.lecun.com/exdb/mnist/). To produce a synthetic sequence of digits for testing, you can for example limit yourself to sequences up to five digits, and use five classifiers on top of your deep network. You would have to incorporate an additional ‘blank’ character to account for shorter number sequences.\n",
    "\n",
    "There are various aspects to consider when thinking about this problem:\n",
    "- Your model can be derived from a deep neural net or a convolutional network.\n",
    "- You could experiment sharing or not the weights between the softmax classifiers.\n",
    "- You can also use a recurrent network in your deep neural net to replace the classification layers and directly emit the sequence of digits one-at-a-time.\n",
    "\n",
    "You can use ** Keras ** to implement your model. Read more at [keras.io](https://keras.io/).\n",
    "\n",
    "Here is an example of a [published baseline model on this problem](http://static.googleusercontent.com/media/research.google.com/en//pubs/archive/42241.pdf). ([video](https://www.youtube.com/watch?v=vGPI_JvLoN0)). You are not expected to model your architecture precisely using this model nor get the same performance levels, but this is more to show an exampe of an approach used to solve this particular problem. We encourage you to try out different architectures for yourself and see what works best for you. Here is a useful [forum post](https://discussions.udacity.com/t/goodfellow-et-al-2013-architecture/202363) discussing the architecture as described in the paper and here is [another one](https://discussions.udacity.com/t/what-loss-function-to-use-for-multi-digit-svhn-training/176897) discussing the loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "Use the code cell (or multiple code cells, if necessary) to implement the first step of your project. Once you have completed your implementation and are satisfied with the results, be sure to thoroughly answer the questions that follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n",
      "Training set (200000, 784) (200000, 10)\n",
      "Validation set (10000, 784) (10000, 10)\n",
      "Test set (10000, 784) (10000, 10)\n",
      "\n",
      "step#       Loss     Training Accuracy Validation Accuracy\n",
      "    0   13228.33            6.25           36.94\n",
      "  500    4624.13           83.59           82.99\n",
      " 1000    1695.38           82.81           83.41\n",
      " 1500     621.40           82.81           83.09\n",
      " 2000     228.41           85.16           85.43\n",
      " 2500      84.29           85.16           88.46\n",
      "Test accuracy: 93.5\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "\n",
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)\n",
    "\n",
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "# helper function to compute accuracy given predictions and labels\n",
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])\n",
    "# helper function to generate a convolution of size nxm with a stride of nxm for input data and its weights\n",
    "def conv2d(x, w, n, m):\n",
    "  return tf.nn.conv2d(x, w, strides=[1, n, m, 1], padding='SAME')\n",
    "\n",
    "# helper function to generate a maxpool rendition of the input data\n",
    "def maxpool(x, n, m):\n",
    "  return tf.nn.max_pool(x, ksize=[1, n, m, 1], strides=[1, n, m, 1], padding='SAME')\n",
    "\n",
    "#helper function to build a weight variable of given sha  pe\n",
    "def weight_variable(shape):\n",
    "  initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "  return tf.Variable(initial)\n",
    "#helper function to build \n",
    "def bias_variable(shape):\n",
    "  initial = tf.constant(0.1, shape=shape)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "def create_logits (X, weights, biases, dropout=False):\n",
    "    #initialize relu_logits as the input dataset\n",
    "    nlayer = len(weights.keys())\n",
    "    cur_logits = X\n",
    "    for layer in range(nlayer):\n",
    "        logits = tf.matmul(cur_logits, weights[layer]) + biases[layer]\n",
    "        # Transform logits computed from layer 1 using relu function to compute logits for layer2\n",
    "        if layer < nlayer-1:\n",
    "            cur_logits = tf.nn.relu(logits)\n",
    "        # Dropout on hidden layers only\n",
    "            if dropout:\n",
    "                cur_logits = tf.nn.dropout(cur_logits, 0.75)\n",
    "\n",
    "    # Apply activation function to transform logits from the last layer to generate predictions for training samples\n",
    "    y_pred = tf.nn.softmax(logits)\n",
    "    return y_pred, logits\n",
    "\n",
    "# helper function to flatten 2d images to 1d tensor\n",
    "def reformat(dataset, labels):\n",
    "    dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "    # Map 2 to [0.0, 1.0, 0.0 ...], 3 to [0.0, 0.0, 1.0 ...]\n",
    "    labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "    return dataset, labels\n",
    "\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)\n",
    "\n",
    "\n",
    "#Create a single hidden layer neural network using RELU and 1024 nodes\n",
    "def n_layer_nn (X_train, y_train, X_valid, y_valid, X_test, y_test, image, num_classes, \n",
    "  batch_size=128, num_nodes=[1024], num_samples=0, num_steps = 1001, print_steps=100, dropout=False):\n",
    "  beta = .01\n",
    "  init_learning_rate = 0.1\n",
    "  if num_samples ==0:\n",
    "      num_samples = y_train.shape[0]\n",
    "\n",
    "  graph = tf.Graph()\n",
    "  #build nodes array and append it with the number of classes which is final number of nodes\n",
    "  nodes_list = num_nodes\n",
    "  nodes_list.append(num_classes)\n",
    "\n",
    "  with graph.as_default():\n",
    "\n",
    "      # Input data. For the training data, we use a placeholder that will be fed\n",
    "      # at run time with a training minibatch.\n",
    "      tf_X_train = tf.placeholder(tf.float32, shape=(batch_size, image[0] * image[1]))\n",
    "      tf_y_train = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "\n",
    "      tf_X_valid = tf.constant(X_valid)\n",
    "      tf_X_test  = tf.constant(X_test)\n",
    "      # save weights and biases in a dictionary for each layer\n",
    "      weights={}\n",
    "      biases ={}\n",
    "      # first dimesion of the weights will be the number of features in the training dataset which is the image size\n",
    "      dim1 = image[0] * image[1]\n",
    "      # build weights and biases for each layer\n",
    "      for i, nodes in enumerate(nodes_list):\n",
    "          weights[i] = tf.Variable(tf.truncated_normal([dim1, nodes]))\n",
    "          biases[i]  = tf.Variable(tf.zeros([nodes]))\n",
    "          w_l2_loss = tf.nn.l2_loss(weights[i])\n",
    "          regularizers = regularizers + w_l2_loss if  i else w_l2_loss\n",
    "          # Subsequent layer first dimension would the number of nodes in the previous layer\n",
    "          dim1 = nodes\n",
    "\n",
    "      y_train_pred, logits = create_logits (tf_X_train, weights, biases, dropout)\n",
    "\n",
    "      # compute the loss comparing logits from layer 2 and the training labels\n",
    "      cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf_y_train, logits=logits))\n",
    "      # Loss function with L2 Regularization with beta\n",
    "      loss = tf.reduce_mean(cross_entropy + beta * regularizers)\n",
    "\n",
    "      # Optimize using Gradient Descent\n",
    "      global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "      learning_rate = tf.train.exponential_decay(init_learning_rate, global_step, decay_steps=100000, decay_rate=0.96, staircase=True)\n",
    "      optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "      \n",
    "      # Transform validation samples using the same transformation used above for training samples\n",
    "      y_valid_pred, _ = create_logits (tf_X_valid, weights, biases)\n",
    "      \n",
    "      # Transform test samples using the same transformation used above for training samples\n",
    "      y_test_pred, _ = create_logits (tf_X_test, weights, biases)\n",
    "\n",
    "\n",
    "  with tf.Session(graph=graph) as session:\n",
    "      # This is a one-time operation which ensures the parameters get initialized as\n",
    "      # we described in the graph: random weights for the matrix, zeros for the\n",
    "      # biases. \n",
    "      tf.global_variables_initializer().run()\n",
    "      print ('\\nstep#       Loss     Training Accuracy Validation Accuracy')\n",
    "      for step in range(num_steps):\n",
    "          offset = (step * batch_size) % (num_samples - batch_size)\n",
    "          # Generate a minibatch from the training dataset for Stocastic Gradient Descent\n",
    "          batch_X = X_train[offset:(offset + batch_size), :]\n",
    "          batch_y = y_train[offset:(offset + batch_size), :]\n",
    "          # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "          # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "          # and the value is the numpy array to feed to it.\n",
    "          feed_dict = {tf_X_train : batch_X, tf_y_train : batch_y}\n",
    "          _, l, y_pred = session.run([optimizer, loss, y_train_pred], feed_dict=feed_dict)\n",
    "\n",
    "          # Run the computations. We tell .run() that we want to run the optimizer,\n",
    "          # and get the loss value and the training predictions returned as numpy\n",
    "          # arrays.\n",
    "          if (step % print_steps == 0):\n",
    "              print('{0:5d} {1:10.2f} {2:15.2f} {3:15.2f}'.format(step, l, accuracy(y_pred, batch_y), \n",
    "                                                            accuracy(y_valid_pred.eval(), y_valid)))\n",
    "              # Calling .eval() on valid_prediction is basically like calling run(), but\n",
    "              # just to get that one numpy array. Note that it recomputes all its graph\n",
    "              # dependencies.\n",
    "      print('Test accuracy: {:.1f}'.format(accuracy(y_test_pred.eval(), y_test))) \n",
    "batch_size = 128\n",
    "num_nodes= [4096]\n",
    "image = [image_size,image_size]\n",
    "nclass = 10\n",
    "num_steps = 2501\n",
    "print_steps = 500\n",
    "dropout = False\n",
    "num_samples=0 # try with full dataset first\n",
    "n_layer_nn (train_dataset, train_labels, valid_dataset, valid_labels, test_dataset, test_labels, image, \n",
    "  nclass, batch_size, num_nodes, num_samples, num_steps, print_steps, dropout)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST Training set (200000, 28, 28) (200000,)\n",
      "MNIST Validation set (10000, 28, 28) (10000,)\n",
      "MNIST Test set (10000, 28, 28) (10000,)\n",
      "MNIST Training set (200000, 28, 28, 1) (200000, 10)\n",
      "MNIST Validation set (10000, 28, 28, 1) (10000, 10)\n",
      "MNIST Testing set (10000, 28, 28, 1) (10000, 10)\n",
      "shape before conv layer: (128, 28, 28, 1) (5, 5, 1, 16)\n",
      "shape after conv layer: (128, 14, 14, 16)\n",
      "shape after maxpool layer: (128, 7, 7, 16)\n",
      "shape before conv layer: (128, 7, 7, 16) (5, 5, 16, 16)\n",
      "shape after conv layer: (128, 4, 4, 16)\n",
      "shape after maxpool layer: (128, 2, 2, 16)\n",
      "shape for fully connected relu layer: (128, 64) (64, 128)\n",
      "shape for fully connected output layer: (128, 128) (128, 10)\n",
      "shape before conv layer: (10000, 28, 28, 1) (5, 5, 1, 16)\n",
      "shape after conv layer: (10000, 14, 14, 16)\n",
      "shape after maxpool layer: (10000, 7, 7, 16)\n",
      "shape before conv layer: (10000, 7, 7, 16) (5, 5, 16, 16)\n",
      "shape after conv layer: (10000, 4, 4, 16)\n",
      "shape after maxpool layer: (10000, 2, 2, 16)\n",
      "shape for fully connected relu layer: (10000, 64) (64, 128)\n",
      "shape for fully connected output layer: (10000, 128) (128, 10)\n",
      "shape before conv layer: (10000, 28, 28, 1) (5, 5, 1, 16)\n",
      "shape after conv layer: (10000, 14, 14, 16)\n",
      "shape after maxpool layer: (10000, 7, 7, 16)\n",
      "shape before conv layer: (10000, 7, 7, 16) (5, 5, 16, 16)\n",
      "shape after conv layer: (10000, 4, 4, 16)\n",
      "shape after maxpool layer: (10000, 2, 2, 16)\n",
      "shape for fully connected relu layer: (10000, 64) (64, 128)\n",
      "shape for fully connected output layer: (10000, 128) (128, 10)\n",
      "Initialized\n",
      "\n",
      "step#       Loss     Training Accuracy Validation Accuracy\n",
      "    0       3.76           10.16            9.57\n",
      "  500       1.30           76.56           75.81\n",
      " 1000       1.29           77.34           79.35\n",
      " 1500       1.14           75.78           81.31\n",
      " 2000       1.11           74.22           82.28\n",
      " 2500       0.87           78.91           82.88\n",
      " 3000       0.89           78.12           82.64\n",
      " 3500       0.88           79.69           82.92\n",
      " 4000       0.94           78.12           83.19\n",
      " 4500       0.82           80.47           83.02\n",
      " 5000       0.58           89.84           83.14\n",
      "Test accuracy: 89.0%\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range\n",
    "import scipy.io as sio\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def reformat(X, Y, image_size, num_channels, num_labels):\n",
    "\tx = np.zeros((X.shape[3],image_size,image_size,num_channels),dtype=np.float32)\n",
    "\tfor i in range(X.shape[3]):\n",
    "\t\tx[i,] = X[:,:,:,i]\n",
    "\ty = (np.arange(num_labels) == Y[:,None]).astype(np.float32)\n",
    "\treturn x, y\n",
    "\n",
    "def reformat_mnist(dataset, labels, image_size, num_channels, num_labels):\n",
    "\tdataset = dataset.reshape((-1, image_size, image_size, num_channels)).astype(np.float32)\n",
    "\tlabels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "\treturn dataset, labels\n",
    "\n",
    "mnist_data = True\n",
    "if mnist_data:\n",
    "\tpickle_file = 'notMNIST.pickle'\n",
    "\n",
    "\twith open(pickle_file, 'rb') as f:\n",
    "\t\tsave = pickle.load(f)\n",
    "\t\ttrain_dataset = save['train_dataset']\n",
    "\t\ttrain_labels = save['train_labels']\n",
    "\t\tvalid_dataset = save['valid_dataset']\n",
    "\t\tvalid_labels = save['valid_labels']\n",
    "\t\ttest_dataset = save['test_dataset']\n",
    "\t\ttest_labels = save['test_labels']\n",
    "\t\tdel save  # hint to help gc free up memory\n",
    "\t\tprint('MNIST Training set', train_dataset.shape, train_labels.shape)\n",
    "\t\tprint('MNIST Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "\t\tprint('MNIST Test set', test_dataset.shape, test_labels.shape)\n",
    "\n",
    "\timage_size   = train_dataset.shape[1] #image size\n",
    "\tnum_labels   = 10 #TODO: number of different label types\n",
    "\tnum_channels = 1 #set as 1:grayscale, or 3: color (RGB)\n",
    "\n",
    "\tX_train, y_train = reformat_mnist(train_dataset, train_labels, image_size, num_channels, num_labels)\n",
    "\tX_valid, y_valid = reformat_mnist(valid_dataset, valid_labels, image_size, num_channels, num_labels)\n",
    "\tX_test,  y_test  = reformat_mnist(test_dataset,  test_labels,  image_size, num_channels, num_labels)\n",
    "\n",
    "\tprint('MNIST Training set',   X_train.shape, y_train.shape)\n",
    "\tprint('MNIST Validation set', X_valid.shape, y_valid.shape)\n",
    "\tprint('MNIST Testing set',    X_test.shape,  y_test.shape)\n",
    "else:\n",
    "\ttrain_data = sio.loadmat('train_32x32.mat')\n",
    "\ttest_data  = sio.loadmat('test_32x32.mat')\n",
    "\n",
    "\tX_data = train_data['X']\n",
    "\ty_data = train_data['y'].reshape(-1)\n",
    "\tX_test = test_data['X']\n",
    "\ty_test = test_data['y'].reshape(-1)\n",
    "\n",
    "\tprint('Real Data     X Shape,       Y_shape')\n",
    "\tprint('Training', X_data.shape, y_data.shape)\n",
    "\tprint('Testing ', X_test.shape, y_test.shape)\n",
    "\n",
    "\timage_size   = X_data.shape[0] #TODO: image size\n",
    "\tnum_labels   = 10 #TODO: number of different label types\n",
    "\tnum_channels = X_data.shape[2] #TODO: set as 1:grayscale, or 3: color (RGB)\n",
    "\n",
    "\t# reshape data for input to CNN model\n",
    "\tX_data, y_data = reformat(X_data, y_data, image_size, num_channels, num_labels)\n",
    "\tX_test, y_test = reformat(X_test, y_test, image_size, num_channels, num_labels)\n",
    "\n",
    "\t# normalize X values between 0 to 1\n",
    "\tX_data = X_data/255\n",
    "\tX_test = X_test/255\n",
    "\n",
    "\t#split X_data, y_data into two chunks of training and validation data\n",
    "\tntrain = round(.8*X_data.shape[0])\n",
    "\tX_train = X_data[:ntrain]\n",
    "\ty_train = y_data[:ntrain]\n",
    "\tX_valid = X_data[ntrain:]\n",
    "\ty_valid = y_data[ntrain:]\n",
    "\n",
    "\tprint('Training  ', X_train.shape, y_train.shape)\n",
    "\tprint('Validation', X_valid.shape, y_valid.shape)\n",
    "\tprint('Testing   ', X_test.shape,  y_test.shape)\n",
    "\n",
    "def accuracy(predictions, labels):\n",
    "\treturn (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1)) / predictions.shape[0])\n",
    "\n",
    "def conv2d(x, W):\n",
    "\treturn tf.nn.conv2d(x, W, strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "\treturn tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "batch_size = 128\n",
    "patch_size = 5\n",
    "depth      = 16\n",
    "num_hidden = 128\n",
    "maxpool    = True\n",
    "dropout    = True\n",
    "# build a convolutional model for the given input data, weights and biases\n",
    "def model(X, weights, biases, maxpool=False, dropout=False):\n",
    "\tnlayer = len(weights.keys())\n",
    "\thidden = X\n",
    "\tfor layer in range(nlayer-2):\n",
    "\t\tprint(\"shape before conv layer:\", hidden.get_shape(), weights[layer].get_shape())\n",
    "\t\thidden = tf.nn.relu(conv2d(hidden, weights[layer]) + biases[layer])\n",
    "\t\tprint(\"shape after conv layer:\", hidden.get_shape())\n",
    "\t\tif maxpool:\n",
    "\t\t\thidden = max_pool_2x2(hidden)\n",
    "\t\t\tprint(\"shape after maxpool layer:\", hidden.get_shape())\n",
    "\t\tif (dropout):\n",
    "\t\t\thidden = tf.nn.dropout(hidden, .8)\n",
    "\n",
    "\tshape  = hidden.get_shape().as_list()\n",
    "\thidden = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "\tprint(\"shape for fully connected relu layer:\", hidden.get_shape(), weights[nlayer-2].get_shape())\n",
    "\thidden = tf.nn.relu(tf.matmul(hidden, weights[nlayer-2]) + biases[nlayer-2])\n",
    "\tprint(\"shape for fully connected output layer:\", hidden.get_shape(), weights[nlayer-1].get_shape())\n",
    "\tlogits = tf.matmul(hidden, weights[nlayer-1]) + biases[nlayer-1]\n",
    "\treturn logits\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\t# Input data.\n",
    "\ttf_X_train = tf.placeholder(tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "\ttf_y_train = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "\ttf_X_valid = tf.constant(X_valid)\n",
    "\ttf_X_test  = tf.constant(X_test)\n",
    "\n",
    "\t# Variables.\n",
    "\tweights={}\n",
    "\tbiases ={}\n",
    "\t# first dimesion of the weights will be the number of features in the training dataset which is the image size\n",
    "\t# build weights and biases for each layer\n",
    "\n",
    "\tweights[0] = tf.Variable(tf.truncated_normal([patch_size, patch_size, num_channels, depth], stddev=0.1))\n",
    "\tbiases[0]  = tf.Variable(tf.zeros([depth]))\n",
    "\tweights[1] = tf.Variable(tf.truncated_normal([patch_size, patch_size, depth, depth], stddev=0.1))\n",
    "\tbiases[1]  = tf.Variable(tf.constant(1.0, shape=[depth]))\n",
    "\tdim1       = depth*16*4\n",
    "\tif maxpool:\n",
    "\t\tdim1 = dim1//16\n",
    "\tweights[2] = tf.Variable(tf.truncated_normal([dim1, num_hidden], stddev=0.1))\n",
    "\tbiases[2]  = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n",
    "\tweights[3] = tf.Variable(tf.truncated_normal([num_hidden, num_labels], stddev=0.1))\n",
    "\tbiases[3]  = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "\n",
    "\t# Build CNN model using specified weights with option to maxpool and dropout\n",
    "\tlogits = model(tf_X_train, weights, biases, maxpool, dropout)\n",
    "\n",
    "\t# Training computation.\n",
    "\tloss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf_y_train, logits=logits))\n",
    "\t# Loss function with L2 Regularization with beta\n",
    "\tl2_loss = True\n",
    "\tif l2_loss:\n",
    "\t\tbeta = .01\n",
    "\t\tnlayer = len(weights.keys())\n",
    "\t\tfor layer in range(nlayer):\n",
    "\t\t\tw_l2_loss = tf.nn.l2_loss(weights[layer])\n",
    "\t\t\tregularizers = regularizers + w_l2_loss if  layer else w_l2_loss\n",
    "\t\tloss = tf.reduce_mean(loss + beta * regularizers)\n",
    "\n",
    "\t# Build optimizer with exponential decay\n",
    "\t#optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n",
    "\tinit_learning_rate = 0.05\n",
    "\tglobal_step = tf.Variable(0)  # count the number of steps taken.\n",
    "\tlearning_rate = tf.train.exponential_decay(init_learning_rate, global_step, decay_steps=100000, decay_rate=0.96, staircase=True)\n",
    "\toptimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "\n",
    "\t# Predictions for the training, validation, and test data.\n",
    "\ttrain_prediction = tf.nn.softmax(logits)\n",
    "\tvalid_prediction = tf.nn.softmax(model(tf_X_valid, weights, biases, maxpool, dropout))\n",
    "\ttest_prediction = tf.nn.softmax(model(tf_X_test, weights, biases, maxpool, dropout))\n",
    "\n",
    "num_steps = 5001\n",
    "print_steps = 500\n",
    "with tf.Session(graph=graph) as session:\n",
    "\ttf.global_variables_initializer().run()\n",
    "\tprint('Initialized')\n",
    "\tprint ('\\nstep#       Loss     Training Accuracy Validation Accuracy')\n",
    "\tfor step in range(num_steps):\n",
    "\t\toffset = (step * batch_size) % (y_train.shape[0] - batch_size)\n",
    "\t\tbatch_data = X_train[offset:(offset + batch_size), :, :, :]\n",
    "\t\tbatch_labels = y_train[offset:(offset + batch_size), :]\n",
    "\t\tfeed_dict = {tf_X_train : batch_data, tf_y_train : batch_labels}\n",
    "\t\t_, l, predictions = session.run(\n",
    "\t\t[optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "\t\tif (step % print_steps == 0):\n",
    "\t\t\tprint('{0:5d} {1:10.2f} {2:15.2f} {3:15.2f}'.format(step, l, accuracy(predictions, batch_labels),\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\taccuracy(valid_prediction.eval(), y_valid)))\n",
    "\tprint('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Convolution2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "import scipy.io as sio\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def reformat(X, Y, image_size, num_channels, num_labels):\n",
    "\tx = np.zeros((X.shape[3],image_size,image_size,num_channels),dtype=np.float32)\n",
    "\tfor i in range(X.shape[3]):\n",
    "\t\tx[i,] = X[:,:,:,i]\n",
    "\ty = (np.arange(num_labels) == Y[:,None]).astype(np.float32)\n",
    "\treturn x, y\n",
    "\n",
    "def load_digits_dataset(mnist_data=True):\n",
    "\tif mnist_data:\n",
    "\t\tK.set_image_dim_ordering('th')\n",
    "\t\t# load data\n",
    "\t\t(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "\t\tprint('Data Set      X Shape,       Y_shape')\n",
    "\t\tprint('Training', X_train.shape, y_train.shape)\n",
    "\t\tprint('Testing ', X_test.shape, y_test.shape)\n",
    "\n",
    "\t\t# reshape to be [samples][pixels][width][height]\n",
    "\t\tX_train = X_train.reshape(X_train.shape[0], 1, 28, 28).astype('float32')\n",
    "\t\tX_test = X_test.reshape(X_test.shape[0], 1, 28, 28).astype('float32')\n",
    "\n",
    "\t\tprint('Training', X_train.shape, y_train.shape)\n",
    "\t\tprint('Testing ', X_test.shape, y_test.shape)\n",
    "\n",
    "\t\t# normalize inputs from 0-255 to 0-1\n",
    "\t\tX_train = X_train / 255\n",
    "\t\tX_test = X_test / 255\n",
    "\t\t# one hot encode outputs\n",
    "\t\ty_train = np_utils.to_categorical(y_train)\n",
    "\t\ty_test = np_utils.to_categorical(y_test)\n",
    "\t\tnum_classes = y_test.shape[1]\n",
    "\t\tprint('\\nFinal Data      X Shape,       Y_shape')\n",
    "\t\tprint('Training', X_train.shape, y_train.shape)\n",
    "\t\tprint('Testing ', X_test.shape, y_test.shape)\n",
    "\t\tinput_shape = (1, 28, 28)\n",
    "\t\tprint ('input shape', input_shape)\n",
    "\telse:\n",
    "\t\tK.set_image_dim_ordering('tf')\n",
    "\t\ttrain_data = sio.loadmat('train_32x32.mat')\n",
    "\t\ttest_data  = sio.loadmat('test_32x32.mat')\n",
    "\n",
    "\t\tX_train = train_data['X']\n",
    "\t\ty_train = train_data['y'].reshape(-1)\n",
    "\t\tX_test = test_data['X']\n",
    "\t\ty_test = test_data['y'].reshape(-1)\n",
    "\n",
    "\t\tprint('\\n\\nReal Set      X Shape,       Y_shape')\n",
    "\t\tprint('Training', X_train.shape, y_train.shape)\n",
    "\t\tprint('Testing ', X_test.shape, y_test.shape)\n",
    "\n",
    "\t\timage_size   = X_train.shape[0] #image size\n",
    "\t\tnum_channels = X_train.shape[2] #set as 1:grayscale, or 3: color (RGB)\n",
    "\t\tnum_classes = 10 # number of types of digits (0-9)\n",
    "\n",
    "\t\t# reshape to be [samples][pixels][width][height]\n",
    "\t\t# reshape data for input to CNN model\n",
    "\n",
    "\t\tX_train, y_train = reformat(X_train, y_train, image_size, num_channels, num_classes)\n",
    "\t\tX_test, y_test   = reformat(X_test, y_test, image_size, num_channels, num_classes)\n",
    "\t\t# normalize X values between 0 to 1\n",
    "\t\tX_train = X_train / 255\n",
    "\t\tX_test = X_test / 255\n",
    "\t\tprint('\\nFinal Data      X Shape,       Y_shape')\n",
    "\t\tprint('Training', X_train.shape, y_train.shape)\n",
    "\t\tprint('Testing ', X_test.shape, y_test.shape)\n",
    "\t\tinput_shape = (image_size, image_size, num_channels)\n",
    "\t\tprint ('input shape', input_shape)\n",
    "\treturn X_train, y_train, X_test, y_test, input_shape, num_classes\n",
    "#========================================================================================\n",
    "def larger_model(input_shape, num_classes):\n",
    "\t# create model\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Convolution2D(32, 5, 5, border_mode='valid', input_shape=input_shape, activation='relu'))\n",
    "\tmodel.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\tmodel.add(Convolution2D(16, 3, 3, activation='relu'))\n",
    "\tmodel.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\tmodel.add(Dropout(0.2))\n",
    "\tmodel.add(Flatten())\n",
    "\tmodel.add(Dense(128, activation='relu'))\n",
    "\tmodel.add(Dense(64, activation='relu'))\n",
    "\tmodel.add(Dense(num_classes, activation='softmax'))\n",
    "\t# Compile model\n",
    "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\treturn model\n",
    "# fix random seed for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Set      X Shape,       Y_shape\n",
      "Training (60000, 28, 28) (60000,)\n",
      "Testing  (10000, 28, 28) (10000,)\n",
      "Training (60000, 1, 28, 28) (60000,)\n",
      "Testing  (10000, 1, 28, 28) (10000,)\n",
      "\n",
      "Final Data      X Shape,       Y_shape\n",
      "Training (60000, 1, 28, 28) (60000, 10)\n",
      "Testing  (10000, 1, 28, 28) (10000, 10)\n",
      "input shape (1, 28, 28)\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/2\n",
      "148s - loss: 0.4533 - acc: 0.8597 - val_loss: 0.0945 - val_acc: 0.9755\n",
      "Epoch 2/2\n",
      "148s - loss: 0.1003 - acc: 0.9693 - val_loss: 0.0649 - val_acc: 0.9825\n",
      "Baseline accuracy: 98.34%\n"
     ]
    }
   ],
   "source": [
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "\n",
    "X_train, y_train, X_test, y_test, input_shape, num_classes = load_digits_dataset (True)\n",
    "# build the model\n",
    "model = larger_model(input_shape, num_classes)\n",
    "# Fit the model using 80% of training data and use remaining 20% for validation\n",
    "n = round(.8*X_train.shape[0])\n",
    "model.fit(X_train[:n], y_train[:n], validation_data=(X_train[n:], y_train[n:]), nb_epoch=2, batch_size=200, verbose=2)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Baseline accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "_What approach did you take in coming up with a solution to this problem?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** \n",
    "\n",
    "\n",
    "As shown in the code cells above, I used MNIST/notMNIST dataset to train and validate following three models:\n",
    "1. Two layer neural network with 4096 nodes in the hidden layer. This provided accuracy of 93.5% \n",
    "2. A 4 layer convolutional neural network using TensorFlow, which yielded accuracy of 91.5%. The actual accuracy here was above 93% as well if the solution was iterated over 3000 steps.\n",
    "3. A 4 layer convolutional neural network using Keras/Theano which yielded accuracy of over 98% with just 2 epochs. This solution goes above 99% accuracy with 10 epochs. Hence I chose to use this for further study.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "_What does your final architecture look like? (Type of model, layers, sizes, connectivity, etc.)_\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "** TensorFlow CNN Model **\n",
    "\n",
    "1. The first hidden layer is a convolutional layer called a Convolution2D. The layer has 32 feature maps, which with the size of 5×5 and a rectifier activation function. This is the input layer, expecting images with the structure outline above [pixels][width][height].\n",
    "2. Next we define a pooling layer that takes the max called MaxPooling2D. It is configured with a pool size of 2×2.\n",
    "The next layer is a regularization layer using dropout called Dropout. It is configured to randomly exclude 20% of neurons in the layer in order to reduce overfitting.\n",
    "3. Next is a layer that converts the 2D matrix data to a vector called Flatten. It allows the output to be processed by standard fully connected layers.\n",
    "4. Next a fully connected layer with 128 neurons and rectifier activation function.\n",
    "5. Finally, the output layer has 10 neurons for the 10 classes and a softmax activation function to output probability-like predictions for each class.\n",
    "\n",
    "** Keras CNN Model **\n",
    "\n",
    "1. Convolutional layer with 32 feature maps of size 5×5.\n",
    "2. Pooling layer taking the max over 2*2 patches.\n",
    "3. Convolutional layer with 16 feature maps of size 3×3.\n",
    "4. Pooling layer taking the max over 2*2 patches.\n",
    "5. Dropout layer with a probability of 20%.\n",
    "6. Flatten layer.\n",
    "7. Fully connected layer with 128 neurons and rectifier activation.\n",
    "8. Fully connected layer with 50 neurons and rectifier activation.\n",
    "9. Output layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "_How did you train your model? How did you generate your synthetic dataset?_ Include examples of images from the synthetic data you constructed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "For NN and CNN model with TensorFlow, notMNIST dataset was used, which was already preprocessed and pickled. However, it still needed to be reformatted to feed to dictionary during runtime in TensorFlow. The labels had be one-HOT encoded as well. This data was already normalized. \n",
    "\n",
    "For CNN model using Keras/Theano, I used MNIST data using Keras's native mnist data loading module. This data had to be normalized and reformatted as well. I used 80% of the training data for training the model and used remaining 20% for the validation purposes. See reformatting functions for both notMNIST and MNIST datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nFZnf3nEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x23c03b7ba20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_train[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x23c6ffb44e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_test[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Step 2: Train a Model on a Realistic Dataset\n",
    "Once you have settled on a good architecture, you can train your model on real data. In particular, the [Street View House Numbers (SVHN)](http://ufldl.stanford.edu/housenumbers/) dataset is a good large-scale dataset collected from house numbers in Google Street View. Training on this more challenging dataset, where the digits are not neatly lined-up and have various skews, fonts and colors, likely means you have to do some hyperparameter exploration to perform well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "Use the code cell (or multiple code cells, if necessary) to implement the first step of your project. Once you have completed your implementation and are satisfied with the results, be sure to thoroughly answer the questions that follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Real Set      X Shape,       Y_shape\n",
      "Training (32, 32, 3, 73257) (73257,)\n",
      "Testing  (32, 32, 3, 26032) (26032,)\n",
      "\n",
      "Final Data      X Shape,       Y_shape\n",
      "Training (73257, 32, 32, 3) (73257, 10)\n",
      "Testing  (26032, 32, 32, 3) (26032, 10)\n",
      "input shape (32, 32, 3)\n",
      "Train on 65931 samples, validate on 7326 samples\n",
      "Epoch 1/5\n",
      "369s - loss: 1.4318 - acc: 0.4345 - val_loss: 0.7113 - val_acc: 0.7189\n",
      "Epoch 2/5\n",
      "373s - loss: 0.6477 - acc: 0.7378 - val_loss: 0.4981 - val_acc: 0.7879\n",
      "Epoch 3/5\n",
      "372s - loss: 0.5174 - acc: 0.7780 - val_loss: 0.4258 - val_acc: 0.8084\n",
      "Epoch 4/5\n",
      "372s - loss: 0.4629 - acc: 0.7920 - val_loss: 0.3900 - val_acc: 0.8178\n",
      "Epoch 5/5\n",
      "370s - loss: 0.4265 - acc: 0.8014 - val_loss: 0.3672 - val_acc: 0.8225\n",
      "Test  accuracy: 81.20%\n"
     ]
    }
   ],
   "source": [
    "### Your code implementation goes here.\n",
    "### Feel free to use as many code cells as needed.\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "#using  Street View House Numbers (SVHN) data set (Not MNIST data)\n",
    "X_train, y_train, X_test, y_test, input_shape, num_classes = load_digits_dataset (mnist_data=False)\n",
    "# build the model\n",
    "model = larger_model(input_shape, num_classes)\n",
    "# Fit the model using 80% of training data and use remaining 20% for validation\n",
    "n = round(.9*X_train.shape[0])\n",
    "model.fit(X_train[:n], y_train[:n], validation_data=(X_train[n:], y_train[n:]), nb_epoch=5, batch_size=200, verbose=2)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Test  accuracy: %.2f%%\" % (scores[1]*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "_Describe how you set up the training and testing data for your model. How does the model perform on a realistic dataset?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "I created load_digits_dataset() function to load either the MNIST or the SVHN datasets. The MNIST data was kept in the Theano style format while the SVHN was kept in TensorFlow style format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "_What changes did you have to make, if any, to achieve \"good\" results? Were there any options you explored that made the results worse?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "The NN model with tensor flow performed quite poorly. The introduction of additional layers made the model very sensitive as the gradient descent diverged resulting in NaN for loss. This could be helped by reducing the batch size and learning rate. However ther resulting accuracy was quite bad (in 20%). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "_What were your initial and final results with testing on a realistic dataset? Do you believe your model is doing a good enough job at classifying numbers correctly?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "The CNN model using Keras performed quite well under the circumstances and with just 5 epochs reach accuracy of over 80%. The publication at Standford also confirms similar findings as they could also manage no more than 85% accuracy ever after 50 epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Step 3: Test a Model on Newly-Captured Images\n",
    "\n",
    "Take several pictures of numbers that you find around you (at least five), and run them through your classifier on your computer to produce example results. Alternatively (optionally), you can try using OpenCV / SimpleCV / Pygame to capture live images from a webcam and run those through your classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "Use the code cell (or multiple code cells, if necessary) to implement the first step of your project. Once you have completed your implementation and are satisfied with the results, be sure to thoroughly answer the questions that follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def larger_model(input_shape, num_classes):\n",
    "\t# create model\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Convolution2D(32, 5, 5, border_mode='valid', input_shape=input_shape, activation='relu'))\n",
    "\tmodel.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\tmodel.add(Convolution2D(16, 3, 3, activation='relu'))\n",
    "\tmodel.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\tmodel.add(Dropout(0.5))\n",
    "\tmodel.add(Flatten())\n",
    "\tmodel.add(Dense(2048, activation='relu'))\n",
    "\tmodel.add(Dropout(0.5))\n",
    "\tmodel.add(Dense(512, activation='relu'))\n",
    "\tmodel.add(Dropout(0.5))\n",
    "\tmodel.add(Dense(num_classes, activation='softmax'))\n",
    "\t# Compile model\n",
    "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\treturn model\n",
    "\n",
    "### Your code implementation goes here.\n",
    "### Feel free to use as many code cells as needed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Real Set      X Shape,       Y_shape\n",
      "Training (32, 32, 3, 73257) (73257,)\n",
      "Testing  (32, 32, 3, 26032) (26032,)\n",
      "\n",
      "Final Data      X Shape,       Y_shape\n",
      "Training (73257, 32, 32, 3) (73257, 10)\n",
      "Testing  (26032, 32, 32, 3) (26032, 10)\n",
      "input shape (32, 32, 3)\n",
      "Train on 65931 samples, validate on 7326 samples\n",
      "Epoch 1/10\n",
      "393s - loss: 1.4318 - acc: 0.4345 - val_loss: 0.7113 - val_acc: 0.7189\n",
      "Epoch 2/10\n",
      "387s - loss: 0.6477 - acc: 0.7378 - val_loss: 0.4981 - val_acc: 0.7879\n",
      "Epoch 3/10\n",
      "388s - loss: 0.5174 - acc: 0.7780 - val_loss: 0.4258 - val_acc: 0.8084\n",
      "Epoch 4/10\n",
      "381s - loss: 0.4629 - acc: 0.7920 - val_loss: 0.3900 - val_acc: 0.8178\n",
      "Epoch 5/10\n",
      "385s - loss: 0.4265 - acc: 0.8014 - val_loss: 0.3672 - val_acc: 0.8225\n",
      "Epoch 6/10\n",
      "379s - loss: 0.3980 - acc: 0.8109 - val_loss: 0.3397 - val_acc: 0.8331\n",
      "Epoch 7/10\n",
      "374s - loss: 0.3787 - acc: 0.8166 - val_loss: 0.3444 - val_acc: 0.8295\n",
      "Epoch 8/10\n",
      "374s - loss: 0.3593 - acc: 0.8212 - val_loss: 0.3269 - val_acc: 0.8363\n",
      "Epoch 9/10\n",
      "383s - loss: 0.3469 - acc: 0.8249 - val_loss: 0.3271 - val_acc: 0.8359\n",
      "Epoch 10/10\n",
      "377s - loss: 0.3332 - acc: 0.8297 - val_loss: 0.3118 - val_acc: 0.8393\n",
      "Test  accuracy: 82.94%\n"
     ]
    }
   ],
   "source": [
    "### Your code implementation goes here.\n",
    "### Feel free to use as many code cells as needed.\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "#using  Street View House Numbers (SVHN) data set (Not MNIST data)\n",
    "X_train, y_train, X_test, y_test, input_shape, num_classes = load_digits_dataset (mnist_data=False)\n",
    "# build the model\n",
    "model = larger_model(input_shape, num_classes)\n",
    "# Fit the model using 80% of training data and use remaining 20% for validation\n",
    "n = round(.9*X_train.shape[0])\n",
    "model.fit(X_train[:n], y_train[:n], validation_data=(X_train[n:], y_train[n:]), nb_epoch=10, batch_size=200, verbose=2)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Test  accuracy: %.2f%%\" % (scores[1]*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7\n",
    "_Choose five candidate images of numbers you took from around you and provide them in the report. Are there any particular qualities of the image(s) that might make classification difficult?_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "def get_image(imagefiles):\n",
    "\tnfile = len(imagefiles)\n",
    "\timg = np.zeros((nfile,32,32,3), dtype=np.float32)\n",
    "\tfor i in range(nfile):\n",
    "\t\timg[i,] = cv2.imread(imagefiles[i])\n",
    "\treturn img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAxEAAACPCAYAAACF41bIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x23c029674e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "imagefiles =['digit1.bmp', 'digit2.bmp','digit3.bmp','digit4.bmp','digit5.bmp','digit6.bmp']\n",
    "img = get_image(imagefiles)\n",
    "fig = plt.figure(figsize=(12, 12), dpi=80)\n",
    "for i in range(len(imagefiles)):\n",
    "    plt.subplot(1,6,i+1)\n",
    "    plt.imshow(img[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "I chose these images as they are from around my house and represent various fonts and some are really faded, e.g. third one. The last image is made with flowers in the garden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6, 7, 7, 3, 2, 5], dtype=int64)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict(img)\n",
    "np.argmax(y_pred, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8\n",
    "_Is your model able to perform equally well on captured pictures or a live camera stream when compared to testing on the realistic dataset?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "As evident from the answer above, the model was able to predict all new six images correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: Question 9\n",
    "_If necessary, provide documentation for how an interface was built for your model to load and classify newly-acquired images._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** Leave blank if you did not complete this part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### Step 4: Explore an Improvement for a Model\n",
    "\n",
    "There are many things you can do once you have the basic classifier in place. One example would be to also localize where the numbers are on the image. The SVHN dataset provides bounding boxes that you can tune to train a localizer. Train a regression loss to the coordinates of the bounding box, and then test it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "Use the code cell (or multiple code cells, if necessary) to implement the first step of your project. Once you have completed your implementation and are satisfied with the results, be sure to thoroughly answer the questions that follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "### Your code implementation goes here.\n",
    "### Feel free to use as many code cells as needed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 10\n",
    "_How well does your model localize numbers on the testing set from the realistic dataset? Do your classification results change at all with localization included?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 11\n",
    "_Test the localization function on the images you captured in **Step 3**. Does the model accurately calculate a bounding box for the numbers in the images you found? If you did not use a graphical interface, you may need to investigate the bounding boxes by hand._ Provide an example of the localization created on a captured image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Optional Step 5: Build an Application or Program for a Model\n",
    "Take your project one step further. If you're interested, look to build an Android application or even a more robust Python program that can interface with input images and display the classified numbers and even the bounding boxes. You can for example try to build an augmented reality app by overlaying your answer on the image like the [Word Lens](https://en.wikipedia.org/wiki/Word_Lens) app does.\n",
    "\n",
    "Loading a TensorFlow model into a camera app on Android is demonstrated in the [TensorFlow Android demo app](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/android), which you can simply modify.\n",
    "\n",
    "If you decide to explore this optional route, be sure to document your interface and implementation, along with significant results you find. You can see the additional rubric items that you could be evaluated on by [following this link](https://review.udacity.com/#!/rubrics/413/view)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional Implementation\n",
    "Use the code cell (or multiple code cells, if necessary) to implement the first step of your project. Once you have completed your implementation and are satisfied with the results, be sure to thoroughly answer the questions that follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "### Your optional code implementation goes here.\n",
    "### Feel free to use as many code cells as needed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Documentation\n",
    "Provide additional documentation sufficient for detailing the implementation of the Android application or Python program for visualizing the classification of numbers in images. It should be clear how the program or application works. Demonstrations should be provided. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Write your documentation here._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note**: Once you have completed all of the code implementations and successfully answered each question above, you may finalize your work by exporting the iPython Notebook as an HTML document. You can do this by using the menu above and navigating to  \n",
    "**File -> Download as -> HTML (.html)**. Include the finished document along with this notebook as your submission."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
